# Choose poetry to build the package

build:
  python: "poetry"

# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.3.x-scala2.12"
    spark_conf:
      spark.databricks.delta.preview.enabled: "true"
    spark_env_vars:
      API_KEY_NINJAS: r7ERdL1aEYKBW+S4kgbUAQ==kjLyAm6iKjeXGon6

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_DS3_v2"

  parrudo-cluster: &parrudo-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_D8s_v3"

environments:
  default:
    workflows:
      # Create Workflow DLT to be delopyed First
      - name: "DLT_Quote"
        workflow_type: "pipeline"
        # access_control_list:
        #   - user_name: "d.martins@kigroup.de"
        #     permission_level: "IS_OWNER"
        clusters:
          - label: "default"
            custom_tags:
              project: "quotes_dbx"
              install_date: "2023-10-23"
              # in case to be `single node`
              # num_workers: 1
            autoscale:
              min_workers: 1
              max_workers: 2
              mode: "ENHANCED"
        # job_clusters:
        #   - job_cluster_key: "basic-cluster"
        #     <<: *basic-static-cluster
        development: true
        continous: false
        photon: false
        libraries:
          - notebook:
              path: "/Repos/d.martins@kigroup.de/quotes_dbx/quotes_dbx/pipeline/dlt_quote"
        edition: CORE
        catalog: "catalog_quotes"
        # In case we use Hive Metastore or Mount Point to ADLS / GCP / S3
        # storage: "/path/to/dbfs/"
        target: "quotes_dbx"
        channel: "CURRENT"

        # Create Second Workflow to reference the DLT Deployed
      - name: "quotes_dbx_workflow"
        job_clusters:
          - job_cluster_key: &default_cluster "default"
            <<: *basic-static-cluster
          - job_cluster_key: &parrudo "parrudo"
            <<: *parrudo-cluster

        tasks:
          - task_key: "setup_catalogs"
            job_cluster_key: *default_cluster
            deployment_config:
              no_package: true # we omit using package since code will be shipped directly from the Repo
            # depends_on:
            #   - task_key: "ml"
            notebook_task:
              notebook_path: "/Repos/d.martins@kigroup.de/quotes_dbx/quotes_dbx/create_catalog_quotes"

            # task based on 2.1 API and wheel_task format
          - task_key: "request_quote"
            job_cluster_key: *default_cluster

            # if you're using python_wheel_task, you'll need the entrypoint or main function to be used in setup.py or in pyproject.toml
            python_wheel_task:
              package_name: "quotes_dbx"
              entry_point: "request-quote" # take a look at the setup.py entry_points section for details on how to define an entrypoint
              # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
            depends_on:
              - task_key: "setup_catalogs"

          - task_key: "Run_DLT"
            pipeline_task:
              pipeline_id: "pipeline://DLT_Quote"
            depends_on:
              - task_key: "request_quote"

          ########## Task using spark_python_task( API 2.0 )
          - task_key: "dummy"
            job_cluster_key: *parrudo

            # if you're using spark_python_task, you'll need the __main__ block to start the code execution
            spark_python_task:
              python_file: "file://quotes_dbx/dummy.py"
              # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
