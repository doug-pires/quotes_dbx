# Design Docs

## Overview

Build a **simple** data pipeline that extract famous quotes, process it and save on storage.

- üéØGoals: Deploy the pipeline into Databricks
- ‚ö†Ô∏è Non-Goals : Be a complex pipeline
- üåü Milestone: Follow best practices and principles
- ü´Ç Main Audience: Other interested engineers and I.
- üî• Bonus : YAML file for CI/CD in GitHub Actions or Azure DevOps

## Requirements

| Requirement No | Requirement Summary                                      |
| -------------- | -------------------------------------------------------- |
| 1              | Use dbx                                                  |
| 2              | Use dba (Databricks Asset Bundles)                       |
| 3              | Apply Unit Testing on Python Code                        |
| 4              | Test Coverage be higher than 80%                         |
| 5              | Use Development Mix Mode ( Notebooks and Python Scripts) |
| 6              | Self document code using Docstrings                      |
| 7              | Documentation with MkDocs                                |


## Design Considerations

### Data sources
It's an API hosted by [API Ninjas]([API Ninjas | Build Real Applications with Real Data (api-ninjas.com)](https://api-ninjas.com/)) and the user should sign up to get an API Key

### Data ingestion
Python code to request the quote and each request will get a new random quote. We can consider the  Data Volume small, being not a challenge in that use case.

### Data Storage
For the sake of simplicity will be stored in DBFS

### Data Processing
For further processing we will use [Delta Live Tables]([Configure pipeline settings for Delta Live Tables | Databricks on AWS](https://docs.databricks.com/en/delta-live-tables/settings.html))

- Autoloader will ingest incrementally to a `streaming` Bronze Table, adding some metadata information to track the batch and auditing purposes.
- Silver table we will hash some columns to uniquely identify a quote.
- Gold Tables for aggregation and report purposes.

### Data Consumption
No Data Consumers waiting in downstream tools

### Data Operations
The orchestration will be done by Databricks Workflows

### Data Governance
Out-of-Scope ‚ùå

### Data Security
Out-of-Scope ‚ùå


## Tech Solution

### Workflow
- Draw the Architecture
- Data Assets

### Manage Metadata and Build Process:
- [Poetry](https://python-poetry.org/)
- [dbx](https://dbx.readthedocs.io/en/latest/)
- [Databricks Asset Bundles]([What are Databricks Asset Bundles? | Databricks on AWS](https://docs.databricks.com/en/dev-tools/bundles/index.html))  *check the repo here*

### Python Libraries:
 - pyspark
 - delta-spark
 - databricks-sdk
 - requests
#### Test
 - pytest
 - [chispa]([MrPowers/chispa: PySpark test helper methods with beautiful error messages (github.com)](https://github.com/MrPowers/chispa))
 - pytest-cov
 - pytest-mock

#### Linters
 - isort
 - black
#### Documentation
 - mkdocs
 - mkdocs-material

